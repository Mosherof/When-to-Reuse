{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088ec9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import *\n",
    "from fixed_points import *\n",
    "from viz import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e806064",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 1000\n",
    "bits = 1\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create task\n",
    "task = FlipFlopTask(batch_size=batch_size, T=100, dt=1.0, flip_prob=0.05, bits=bits)\n",
    "U, _ = task.get()\n",
    "\n",
    "# Create RNN\n",
    "rnn = RNN(input_size=task.bits, hidden_size=100, output_size=task.bits, tau=2.0, dt=1.0, use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0287d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN on flip flop task...\n",
      "Epochs: 1000, Learning rate: 0.01, Batch size: 1\n",
      "------------------------------------------------------------\n",
      "Epoch 100/1000, Loss: 0.272948\n",
      "Epoch 200/1000, Loss: 0.080789\n",
      "Epoch 300/1000, Loss: 0.026353\n",
      "Epoch 400/1000, Loss: 0.128781\n",
      "Epoch 500/1000, Loss: 0.042447\n",
      "Epoch 600/1000, Loss: 0.022009\n",
      "Epoch 700/1000, Loss: 0.010337\n",
      "Epoch 800/1000, Loss: 0.007623\n",
      "Epoch 900/1000, Loss: 0.002557\n",
      "Epoch 1000/1000, Loss: 0.001583\n",
      "------------------------------------------------------------\n",
      "Training complete!\n",
      "Shape of R_list: (1000, 1, 101, 100)\n",
      "Shape of R_list after processing: (1000, 101, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train the RNN\n",
    "train_R_list, losses, weight_snapshots = train_rnn(rnn, task, n_epochs=n_epochs, learning_rate=1e-2, batch_size=batch_size)\n",
    "train_R_list = process_R_list(train_R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335dfc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding fixed points for 1000 epochs...\n",
      "  Epoch 100/1000: 1 fixed points\n",
      "  Epoch 200/1000: 1 fixed points\n",
      "  Epoch 300/1000: 1 fixed points\n",
      "  Epoch 400/1000: 1 fixed points\n",
      "  Epoch 500/1000: 0 fixed points\n",
      "  Epoch 600/1000: 0 fixed points\n",
      "  Epoch 700/1000: 0 fixed points\n",
      "  Epoch 800/1000: 0 fixed points\n",
      "  Epoch 900/1000: 1 fixed points\n",
      "  Epoch 1000/1000: 0 fixed points\n",
      "Generating animation with 100 frames...\n",
      "Saving animation to plots/FP_training1_fullpca.mp4...\n",
      "Animation saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# animate fixed points over training, using PCA fitted on all fixed points found across training\n",
    "_ = animate_fixed_points(rnn, train_R_list, save_path=\"plots/FP_training1_fullpca.mp4\", stride=10, weight_snapshots=weight_snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e693985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is incorrect because it uses the current rnn weight to find fixed points, rather than the weights at the time\n",
    "# unique_fps = find_fixed_points_KE_min(rnn, train_R_list[80])\n",
    "# pca, lim = animate_R(train_R_list[80], save_path=\"plots/train_1.mp4\", fps=30, stride=1, fixed_points=unique_fps, title=\"RNN trajectory, first training\", pca=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9b8fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "------------------------------------------------------------\n",
      "Epoch 100/1000, Loss: 0.001186\n",
      "Epoch 200/1000, Loss: 0.001123\n",
      "Epoch 300/1000, Loss: 0.001419\n",
      "Epoch 400/1000, Loss: 0.000827\n",
      "Epoch 500/1000, Loss: 0.001288\n",
      "Epoch 600/1000, Loss: 0.001152\n",
      "Epoch 700/1000, Loss: 0.001635\n",
      "Epoch 800/1000, Loss: 0.001688\n",
      "Epoch 900/1000, Loss: 0.001628\n",
      "Epoch 1000/1000, Loss: 0.001323\n",
      "------------------------------------------------------------\n",
      "Inference complete! Mean loss: 0.001841\n",
      "Shape of R_list: (1000, 1, 101, 100)\n",
      "Shape of R_list after processing: (1000, 101, 100)\n",
      "Finding fixed points for 1000 epochs...\n",
      "  Epoch 100/1000: 0 fixed points\n",
      "  Epoch 200/1000: 0 fixed points\n",
      "  Epoch 300/1000: 0 fixed points\n",
      "  Epoch 400/1000: 0 fixed points\n",
      "  Epoch 500/1000: 0 fixed points\n",
      "  Epoch 600/1000: 0 fixed points\n",
      "  Epoch 700/1000: 0 fixed points\n",
      "  Epoch 800/1000: 0 fixed points\n",
      "  Epoch 900/1000: 0 fixed points\n",
      "  Epoch 1000/1000: 0 fixed points\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m R_list, _, weight_snapshots = inference_rnn(rnn, task)\n\u001b[32m      3\u001b[39m R_list = process_R_list(R_list)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m _ = \u001b[43manimate_fixed_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplots/FP_inference1_fullpca.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_snapshots\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_snapshots\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/neural-comp-proj/viz.py:207\u001b[39m, in \u001b[36manimate_fixed_points\u001b[39m\u001b[34m(rnn, train_R_list, pca, save_path, fps, dpi, show, lim, title, interval, fp_finder, fp_kwargs, stride, weight_snapshots)\u001b[39m\n\u001b[32m    205\u001b[39m     all_fps = np.concatenate([fp \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m fps_per_epoch \u001b[38;5;28;01mif\u001b[39;00m fp.shape[\u001b[32m0\u001b[39m] > \u001b[32m0\u001b[39m], axis=\u001b[32m0\u001b[39m)\n\u001b[32m    206\u001b[39m     pca = PCA(n_components=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_fps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Project all fixed points to 2D\u001b[39;00m\n\u001b[32m    210\u001b[39m fps_2d_per_epoch = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/neural-comp-proj/.venv/lib/python3.13/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/neural-comp-proj/.venv/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:440\u001b[39m, in \u001b[36mPCA.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    424\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[32m    425\u001b[39m \n\u001b[32m    426\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/neural-comp-proj/.venv/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:540\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcovariance_eigh\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_truncated(X, n_components, xp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/neural-comp-proj/.venv/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:554\u001b[39m, in \u001b[36mPCA._fit_full\u001b[39m\u001b[34m(self, X, n_components, xp, is_array_api_compliant)\u001b[39m\n\u001b[32m    550\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    551\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmle\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is only supported if n_samples >= n_features\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    552\u001b[39m         )\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= n_components <= \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    555\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be between 0 and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_ = xp.mean(X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "# animate fixed points over inference\n",
    "R_list, _, weight_snapshots = inference_rnn(rnn, task)\n",
    "R_list = process_R_list(R_list)\n",
    "_ = animate_fixed_points(rnn, R_list, save_path=\"plots/FP_inference1_fullpca.mp4\", stride=10, weight_snapshots=weight_snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing weights: ['W_in', 'W_out', 'b_out']\n",
      "Warning: Cannot reinitialize b_out when use_bias=False\n"
     ]
    }
   ],
   "source": [
    "rnn.reinitialize_weights(weights=['W_in', 'W_out', 'b_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb438e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN on flip flop task...\n",
      "Epochs: 1000, Learning rate: 0.01, Batch size: 1\n",
      "------------------------------------------------------------\n",
      "Epoch 100/1000, Loss: 0.210329\n",
      "Epoch 200/1000, Loss: 0.164822\n",
      "Epoch 300/1000, Loss: 0.056897\n",
      "Epoch 400/1000, Loss: 0.025257\n",
      "Epoch 500/1000, Loss: 0.018018\n",
      "Epoch 600/1000, Loss: 0.008449\n",
      "Epoch 700/1000, Loss: 0.003048\n",
      "Epoch 800/1000, Loss: 0.002201\n",
      "Epoch 900/1000, Loss: 0.009211\n",
      "Epoch 1000/1000, Loss: 0.002102\n",
      "------------------------------------------------------------\n",
      "Training complete!\n",
      "Shape of R_list: (1000, 1, 101, 100)\n",
      "Shape of R_list after processing: (1000, 101, 100)\n",
      "Finding fixed points for 1000 epochs...\n",
      "  Epoch 100/1000: 1 fixed points\n",
      "  Epoch 200/1000: 1 fixed points\n",
      "  Epoch 300/1000: 0 fixed points\n",
      "  Epoch 400/1000: 0 fixed points\n",
      "  Epoch 500/1000: 0 fixed points\n",
      "  Epoch 600/1000: 0 fixed points\n",
      "  Epoch 700/1000: 0 fixed points\n",
      "  Epoch 800/1000: 0 fixed points\n",
      "  Epoch 900/1000: 0 fixed points\n",
      "  Epoch 1000/1000: 0 fixed points\n",
      "Generating animation with 100 frames...\n",
      "Saving animation to plots/FP_training2_fullpca.mp4...\n",
      "Animation saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train the RNN\n",
    "train_R_list, losses, weight_snapshots = train_rnn(rnn, task, n_epochs=n_epochs, learning_rate=1e-2, batch_size=batch_size)\n",
    "train_R_list = process_R_list(train_R_list)\n",
    "\n",
    "_ = animate_fixed_points(rnn, train_R_list, save_path=\"plots/FP_training2_fullpca.mp4\", stride=10, weight_snapshots=weight_snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8434a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
